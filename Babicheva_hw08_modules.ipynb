{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannaBabicheva/hw08_modules/blob/main/Babicheva_hw08_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JpDd6HvFYo0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F875tsqbkPMr"
      },
      "source": [
        "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A_-VhXfZkPMt"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ72jtHGkPMu"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module - это абстрактный класс, который определяет фундаментальные методы, необходимые для обучения нейронной сети. Вам не нужно ничего здесь менять, просто прочитайте комментарии."
      ],
      "metadata": {
        "id": "ZHN5bdJAHHdm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4B88wnO1kPMu"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    В принципе, вы можете думать о модуле как о чем-то (черном ящике).\n",
        "    который может обрабатывать \"входные\" данные и выдавать \"выходные\" данные.\n",
        "    Это похоже на применение функции, которая называется \"переадресация`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    Модуль должен уметь выполнять обратный переход, чтобы отличать функцию \"вперед\".\n",
        "    Более того, он должен уметь отличать ее, если она является частью цепочки (правило цепочки).\n",
        "    Последнее подразумевает, что существует отклонение от предыдущего шага правила цепочки.\n",
        "\n",
        "         gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Принимает входной объект и вычисляет соответствующий выходной сигнал модуля.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "Выполняет шаг обратного распространения по модулю относительно заданного входного сигнала.\n",
        "\n",
        "        Это включает в себя: - вычисление градиента с использованием \"входных данных\" (необходимо для дальнейшей обратной обработки),\n",
        "- вычисление градиента с параметрами (для обновления параметров при оптимизации).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "       Вычисляет выходные данные, используя текущий набор параметров класса и input.\n",
        "        Эта функция возвращает результат, который сохраняется в поле \"output\".\n",
        "\n",
        "        Убедитесь, что данные как сохраняются в поле \"output\", так и возвращаются.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "       Вычисление градиента модуля относительно его собственных входных данных.\n",
        "        Это значение возвращается в `gradInput`. Кроме того, переменная состояния `gradInput` обновляется соответствующим образом.\n",
        "\n",
        "        Форма `gradInput` всегда совпадает с формой `input`.\n",
        "\n",
        "        Убедитесь, что вы сохранили градиенты в поле \"gradInput\" и вернули его.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Вычисление градиента модуля относительно его собственных параметров.\n",
        "        Нет необходимости переопределять, если модуль не имеет параметров (например, ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Обнуляет переменную gradParams, если в модуле есть параметры.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Возвращает список с его параметрами.\n",
        "        Если модуль не имеет параметров, возвращает пустой список.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Возвращает список с градиентами относительно его параметров.\n",
        "        Если модуль не имеет параметров, возвращает пустой список.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Устанавливает режим обучения для модуля.\n",
        "        Поведение при обучении и тестировании отличается в случае отсева и пакетной нормы.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Устанавливает режим оценки для модуля.\n",
        "        Поведение при обучении и тестировании отличается в случае отсева и пакетной нормы.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "       Красиво напечатано. Должно быть переопределено в каждом модуле, если вы хотите\n",
        "иметь читаемое описание.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znSWLRuEkPMv"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InOYWA5UkPMv"
      },
      "source": [
        "Определите процедуры прямого и обратного прохождения."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " прямой и обратный проход для контейнера Sequential, который последовательно обрабатывает данные через слои нейронной сети."
      ],
      "metadata": {
        "id": "aFakzbED9akW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-f4iLs1BYAtZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "    Этот класс реализует контейнер, который последовательно обрабатывает входные данные.\n",
        "\n",
        "    input обрабатывается каждым модулем (слоем) в self.modules последовательно.\n",
        "    Результирующий массив называется output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация Sequential контейнера.\n",
        "        Наследует базовый класс Module и создает пустой список модулей.\n",
        "        \"\"\"\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Добавляет модуль в контейнер.\n",
        "\n",
        "        Args:\n",
        "            module: модуль для добавления в последовательность\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Реализует прямой проход через последовательность модулей.\n",
        "\n",
        "        Схема:\n",
        "        y_0 = module[0].forward(input)\n",
        "        y_1 = module[1].forward(y_0)\n",
        "        ...\n",
        "        output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "        Args:\n",
        "            input: входные данные для первого модуля\n",
        "\n",
        "        Returns:\n",
        "            output: выход последнего модуля\n",
        "        \"\"\"\n",
        "        # Начинаем с входных данных\n",
        "        current_input = input\n",
        "\n",
        "        # Сохраняем промежуточные выходы для обратного прохода\n",
        "        self.intermediates = []\n",
        "\n",
        "        # Проходим через каждый модуль последовательно\n",
        "        for module in self.modules:\n",
        "            # Сохраняем текущий вход для обратного прохода\n",
        "            self.intermediates.append(current_input)\n",
        "\n",
        "            # Пропускаем через текущий модуль\n",
        "            current_input = module.forward(current_input)\n",
        "\n",
        "        # Сохраняем и возвращаем финальный выход\n",
        "        self.output = current_input\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Реализует обратный проход через последовательность модулей.\n",
        "\n",
        "        Схема:\n",
        "        g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "        g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "        ...\n",
        "        g_1 = module[1].backward(y_0, g_2)\n",
        "        gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "        Args:\n",
        "            input: оригинальный вход Sequential модуля\n",
        "            gradOutput: градиент от следующего модуля\n",
        "\n",
        "        Returns:\n",
        "            gradInput: градиент по входу\n",
        "        \"\"\"\n",
        "        # Начинаем с градиента выхода\n",
        "        current_gradient = gradOutput\n",
        "\n",
        "        # Проходим через модули в обратном порядке\n",
        "        for i in range(len(self.modules) - 1, -1, -1):\n",
        "            # Определяем правильный вход для текущего модуля:\n",
        "            # Для первого модуля используем оригинальный вход,\n",
        "            # для остальных - сохраненные промежуточные значения\n",
        "            if i == 0:\n",
        "                current_input = input\n",
        "            else:\n",
        "                current_input = self.intermediates[i]\n",
        "\n",
        "            # Вычисляем градиент через текущий модуль\n",
        "            current_gradient = self.modules[i].backward(current_input, current_gradient)\n",
        "\n",
        "        # Сохраняем и возвращаем финальный градиент\n",
        "        self.gradInput = current_gradient\n",
        "        return self.gradInput\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Обнуляет градиенты параметров всех модулей.\n",
        "        \"\"\"\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Собирает все параметры в список.\n",
        "\n",
        "        Returns:\n",
        "            list: список параметров всех модулей\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Собирает все градиенты параметров в список.\n",
        "\n",
        "        Returns:\n",
        "            list: список градиентов параметров всех модулей\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Строковое представление последовательности модулей.\n",
        "\n",
        "        Returns:\n",
        "            str: строка с описанием всех модулей\n",
        "        \"\"\"\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self, x):\n",
        "        \"\"\"\n",
        "        Получение модуля по индексу.\n",
        "\n",
        "        Args:\n",
        "            x: индекс модуля\n",
        "\n",
        "        Returns:\n",
        "            Module: модуль с указанным индексом\n",
        "        \"\"\"\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Переводит все модули в режим обучения.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Переводит все модули в режим оценки.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ],
      "metadata": {
        "id": "ZY7QPUmiXg6R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Прямой проход:\n",
        "   - Последовательное применение слоев\n",
        "   - Сохранение промежуточных результатов\n",
        "   - Каждый выход становится входом для следующего слоя\n",
        "\n",
        "2. Обратный проход:\n",
        "   - Проход в обратном порядке\n",
        "   - Использование сохраненных промежуточных значений\n",
        "   - Правильная передача градиентов между слоями\n",
        "\n",
        "3. Управление состоянием:\n",
        "   - Режимы train/evaluate\n",
        "   - Сбор параметров и градиентов\n",
        "   - Обнуление градиентов\n",
        "\n",
        "4. Важные моменты:\n",
        "   - Сохранение промежуточных результатов для backward\n",
        "   - Правильная передача входов для каждого слоя\n",
        "   - Поддержка всех необходимых операций (train, eval, параметры)"
      ],
      "metadata": {
        "id": "Bj0-0ATBG6lu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts28FshykPMw"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8zz85rAkPMw"
      },
      "source": [
        "## 1. Слой линейного преобразования\n",
        "Также известен как плотный слой, полносвязанный слой, FC-слой, внутренний продуктовый слой InnerProductLayer (в кофе), аффинная трансформация - affine transform\n",
        "\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    Модуль, который применяет линейное преобразование.\n",
        "    Работает с 2D-входом формы (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # Инициализация весов и смещений\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size=(n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход: y = Wx + b\n",
        "        input: [batch_size, n_feats1]\n",
        "        W: [n_feats2, n_feats1]\n",
        "        b: [n_feats2]\n",
        "        output: [batch_size, n_feats2]\n",
        "        \"\"\"\n",
        "        self.output = np.dot(input, self.W.T) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход по входу.\n",
        "        gradOutput: [batch_size, n_feats2]\n",
        "        W: [n_feats2, n_feats1]\n",
        "        gradInput: [batch_size, n_feats1]\n",
        "        \"\"\"\n",
        "        self.gradInput = np.dot(gradOutput, self.W)\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход по параметрам.\n",
        "        input: [batch_size, n_feats1]\n",
        "        gradOutput: [batch_size, n_feats2]\n",
        "        gradW: [n_feats2, n_feats1]\n",
        "        gradb: [n_feats2]\n",
        "        \"\"\"\n",
        "        # Градиент по весам\n",
        "        self.gradW = np.dot(gradOutput.T, input)\n",
        "\n",
        "        # Градиент по смещениям\n",
        "        self.gradb = np.sum(gradOutput, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1], s[0])\n",
        "        return q"
      ],
      "metadata": {
        "id": "4fkGolaeEQsi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Основные компоненты реализации:\n",
        "\n",
        "1. `updateOutput(self, input)`:\n",
        "- Реализует прямой проход y = Wx + b\n",
        "- Используется np.dot для матричного умножения\n",
        "- Добавляется вектор смещения b\n",
        "- Размерности: input [batch_size, n_feats1] -> output [batch_size, n_feats2]\n",
        "\n",
        "2. `updateGradInput(self, input, gradOutput)`:\n",
        "- Вычисляет градиент по входу\n",
        "- Использует транспонированную матрицу весов\n",
        "- Размерности: gradOutput [batch_size, n_feats2] -> gradInput [batch_size, n_feats1]\n",
        "\n",
        "3. `accGradParameters(self, input, gradOutput)`:\n",
        "- Вычисляет градиенты по параметрам (W и b)\n",
        "- Для весов: gradW = gradOutput^T * input\n",
        "- Для смещений: gradb = sum(gradOutput, axis=0)\n",
        "- Размерности:\n",
        "  - gradW: [n_feats2, n_feats1]\n",
        "  - gradb: [n_feats2]\n",
        "\n",
        "4. Вспомогательные методы:\n",
        "- `zeroGradParameters()`: обнуляет градиенты\n",
        "- `getParameters()`: возвращает текущие параметры\n",
        "- `getGradParameters()`: возвращает градиенты параметров\n",
        "\n",
        "Линейный слой является основным строительным блоком нейронных сетей."
      ],
      "metadata": {
        "id": "7-ncyliJFRr2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4SbSLjAkPMx"
      },
      "source": [
        "## 2. SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход:\n",
        "\n",
        "        softmax(x)_i = exp(x_i) / sum_j(exp(x_j))\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        output: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Нормализация для численной стабильности\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Вычисляем exp() от нормализованных значений\n",
        "        exp_x = np.exp(self.output)\n",
        "\n",
        "        # Делим на сумму по строке для получения вероятностей\n",
        "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход.\n",
        "\n",
        "        Градиент softmax:\n",
        "        d(softmax_i)/d(x_j) = softmax_i * (delta_ij - softmax_j)\n",
        "        где delta_ij = 1 если i=j, иначе 0\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        gradOutput: [batch_size, n_feats]\n",
        "        gradInput: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Получаем значения softmax с прямого прохода\n",
        "        softmax_output = self.output\n",
        "\n",
        "        # Для каждого примера в батче\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "        for i in range(len(gradOutput)):\n",
        "            # Вычисляем якобиан softmax\n",
        "            # diag(softmax) - outer(softmax, softmax)\n",
        "            jacobian = np.diag(softmax_output[i]) - \\\n",
        "                      np.outer(softmax_output[i], softmax_output[i])\n",
        "\n",
        "            # Умножаем градиент на якобиан\n",
        "            self.gradInput[i] = np.dot(gradOutput[i], jacobian)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ],
      "metadata": {
        "id": "eAcsaVGHFKv4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr9VXXsokPMx"
      },
      "source": [
        "## 3. LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss.\n",
        "\n",
        "Основная цель этого уровня - использовать его при вычислении потерь с логарифмическим правдоподобием.\n",
        "\n",
        "LogSoftMax применяет логарифм функции софтмакс к входным данным."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "        super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход: logsoftmax(x)_i = x_i - log(sum_j(exp(x_j)))\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        output: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Нормализация для численной стабильности\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Вычисляем exp() от нормализованных значений\n",
        "        exp_x = np.exp(self.output)\n",
        "\n",
        "        # Вычисляем логарифм суммы экспонент\n",
        "        log_sum_exp = np.log(np.sum(exp_x, axis=1, keepdims=True))\n",
        "\n",
        "        # Финальный результат: x_i - log(sum(exp(x_j)))\n",
        "        self.output = self.output - log_sum_exp\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход.\n",
        "\n",
        "        Градиент logsoftmax:\n",
        "        d(logsoftmax_i)/d(x_j) = delta_ij - softmax_j\n",
        "        где delta_ij = 1 если i=j, иначе 0\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        gradOutput: [batch_size, n_feats]\n",
        "        gradInput: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Сначала вычисляем softmax от входа\n",
        "        # Используем ту же нормализацию\n",
        "        normalized_input = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exp_x = np.exp(normalized_input)\n",
        "        softmax_output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "        # Градиент: gradOutput_i - sum_j(gradOutput_j * softmax_j)\n",
        "        self.gradInput = gradOutput - np.sum(gradOutput, axis=1, keepdims=True) * softmax_output\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ],
      "metadata": {
        "id": "VGh98T1n1r8x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Особенности реализации:\n",
        "1. Используется нормализация для предотвращения численной нестабильности\n",
        "2. Градиент имеет более простую форму, чем у обычного softmax\n",
        "3. LogSoftMax часто используется вместе с NLL (Negative Log Likelihood) loss для задач классификации\n",
        "4. Эта комбинация численно более стабильна, чем использование обычного softmax с cross-entropy loss\n",
        "\n",
        "LogSoftMax часто используется в последнем слое нейронной сети для задач классификации, особенно когда используется функция потерь NLL (отрицательное логарифмическое правдоподобие)."
      ],
      "metadata": {
        "id": "UfeD3MSi4KVY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lmg1FNqkPMx"
      },
      "source": [
        "## 4. Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Batch Normalization\n",
        "\n",
        "\n",
        "Перевод задания:\n",
        "\n",
        "\n",
        "Одна из самых значимых недавних идей, которая сильно повлияла на нейронные сети - это пакетная нормализация. Идея проста, но эффективна: признаки должны быть отбелены (среднее=0, станд=1) на протяжении всей нейронной сети. Это улучшает сходимость для глубоких моделей, позволяя обучать их пару дней, а не недели. Вам нужно реализовать первую часть слоя: нормализацию признаков. Вторая часть (слой `ChannelwiseScaling`) реализована ниже.\n",
        "\n",
        "\n",
        "Вход: `batch_size x n_feats`\n",
        "Выход: `batch_size x n_feats`\n",
        "\n",
        "\n",
        "Слой должен работать следующим образом. Во время обучения (`self.training == True`) он преобразует вход как:\n",
        "\n",
        "$ y = (x - μ) / √(σ + ε) $\n",
        "\n",
        "где μ и σ - среднее и дисперсия значений признаков в батче,\n",
        "\n",
        "а ε - просто маленькое число для численной стабильности.\n",
        "\n",
        "\n",
        "Также во время обучения слой должен поддерживать экспоненциальное скользящее среднее для значений среднего и дисперсии:\n",
        "\n",
        "`'self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)'`\n",
        "\n",
        "\n",
        "`self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)`\n",
        "\n",
        "\n",
        "Во время тестирования (`self.training == False`) слой нормализует вход, используя `moving_mean` и `moving_variance`.\n",
        "\n"
      ],
      "metadata": {
        "id": "03xSL-H34_w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "        self.training = True\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход batch normalization.\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        output: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Инициализируем moving_mean и moving_variance если это первый проход\n",
        "        if self.moving_mean is None:\n",
        "            self.moving_mean = np.zeros(input.shape[1])\n",
        "            self.moving_variance = np.ones(input.shape[1])\n",
        "\n",
        "        # Сохраняем входные данные для обратного прохода\n",
        "        self.input = input\n",
        "\n",
        "        if self.training:\n",
        "            # Вычисляем среднее и дисперсию по батчу\n",
        "            self.batch_mean = np.mean(input, axis=0)\n",
        "            self.batch_var = np.var(input, axis=0)\n",
        "\n",
        "            # Обновляем moving averages\n",
        "            self.moving_mean = self.moving_mean * self.alpha + \\\n",
        "                             self.batch_mean * (1 - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + \\\n",
        "                                 self.batch_var * (1 - self.alpha)\n",
        "\n",
        "            # Нормализуем используя batch statistics\n",
        "            self.x_centered = input - self.batch_mean\n",
        "            self.std = np.sqrt(self.batch_var + self.EPS)\n",
        "            self.output = self.x_centered / self.std\n",
        "\n",
        "        else:\n",
        "            # В режиме тестирования используем moving averages\n",
        "            self.output = (input - self.moving_mean) / \\\n",
        "                         np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход batch normalization.\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        gradOutput: [batch_size, n_feats]\n",
        "        gradInput: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            batch_size = input.shape[0]\n",
        "\n",
        "            # Градиент по x_centered\n",
        "            grad_x_centered = gradOutput / self.std\n",
        "\n",
        "            # Градиент по variance\n",
        "            grad_var = np.sum(gradOutput * self.x_centered * -0.5 * \\\n",
        "                            (self.batch_var + self.EPS)**(-1.5), axis=0)\n",
        "\n",
        "            # Градиент по mean\n",
        "            grad_mean = np.sum(gradOutput * -1.0 / self.std, axis=0) + \\\n",
        "                       grad_var * np.mean(-2.0 * self.x_centered, axis=0)\n",
        "\n",
        "            # Градиент по input\n",
        "            self.gradInput = gradOutput / self.std + \\\n",
        "                            grad_var * 2.0 * self.x_centered / batch_size + \\\n",
        "                            grad_mean / batch_size\n",
        "        else:\n",
        "            # В режиме тестирования просто масштабируем градиент\n",
        "            self.gradInput = gradOutput / \\\n",
        "                            np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "metadata": {
        "id": "xhipzIVS6U6j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "i2JkygG9kPMx"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Реализует линейное преобразование входных данных  y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - обучаемые векторы длины  x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные компоненты реализации:\n",
        "\n",
        "1. `updateOutput(self, input)`:\n",
        "   - В режиме обучения:\n",
        "     - Вычисляет среднее и дисперсию по батчу\n",
        "     - Обновляет экспоненциальные скользящие средние\n",
        "     - Нормализует данные используя статистики батча\n",
        "   - В режиме тестирования:\n",
        "     - Использует накопленные moving averages для нормализации\n",
        "\n",
        "2. `updateGradInput(self, input, gradOutput)`:\n",
        "   - В режиме обучения:\n",
        "     - Вычисляет градиенты по всем компонентам (mean, variance, input)\n",
        "     - Учитывает все зависимости при обратном распространении\n",
        "   - В режиме тестирования:\n",
        "     - Просто масштабирует градиент используя moving variance\n",
        "\n",
        "Этот слой значительно улучшает обучение глубоких нейронных сетей, уменьшая internal covariate shift и позволяя использовать более высокие learning rates"
      ],
      "metadata": {
        "id": "NzjgebYzE9fs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbh8aNFkPMy"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Практические заметки. Если BatchNormalization  размещена после слоя линейного преобразования (включая плотный слой, свертки, масштабирование по каналам), который реализует функцию типа `y = weight * x + bias`, то добавление смещения становится бесполезным и может быть опущено, поскольку его эффект будет отброшен при вычитании пакетного среднего. Если пакетная нормализация (за которой следует масштабирование по каналам) помещается перед слоем, который увеличивает масштаб (включая `ReLU, LeakyReLU`), за которым следует любой слой линейного преобразования, то параметр `gamma` в масштабировании по каналам может быть заблокирован, поскольку он может быть поглощен слоем линейного преобразования."
      ],
      "metadata": {
        "id": "mY7h_1yvEETF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnDPUhaOkPMy"
      },
      "source": [
        "## 5. Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перевод задания:\n",
        "Реализуйте `dropout`. Идея и реализация действительно просты: просто умножьте вход на маску $Бернулли(p)$. Здесь $p$ - вероятность того, что элемент будет обнулен.\n",
        "\n",
        "Это доказало свою эффективность как метод регуляризации и предотвращения коадаптации нейронов.\n",
        "\n",
        "Во время обучения (`self.training == True`) он должен сэмплировать маску на каждой итерации (для каждого батча), обнулять элементы и умножать элементы на $ 1/(1-p)$. Последнее необходимо для сохранения средних значений признаков близкими к средним значениям, которые будут в тестовом режиме. При тестировании этот модуль должен реализовывать тождественное преобразование, т.е. `self.output = input.`\n",
        "\n",
        "*   Вход: `batch_size x n_feats`\n",
        "*   Выход: `batch_size x n_feats`\n"
      ],
      "metadata": {
        "id": "q_fU54MbFRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        \"\"\"\n",
        "        p: вероятность обнуления элемента\n",
        "        \"\"\"\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.training = True\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход dropout.\n",
        "\n",
        "        В режиме обучения:\n",
        "        - Создает маску Бернулли\n",
        "        - Умножает вход на маску\n",
        "        - Масштабирует на 1/(1-p)\n",
        "\n",
        "        В режиме тестирования:\n",
        "        - Возвращает вход без изменений\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        output: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            # Генерируем маску Бернулли (1-p) - вероятность сохранить элемент\n",
        "            self.mask = np.random.binomial(1, 1-self.p, size=input.shape)\n",
        "\n",
        "            # Масштабируем на 1/(1-p) для сохранения математического ожидания\n",
        "            self.output = input * self.mask / (1 - self.p)\n",
        "        else:\n",
        "            # В режиме тестирования просто копируем вход\n",
        "            self.output = input\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход dropout.\n",
        "\n",
        "        В режиме обучения:\n",
        "        - Умножает градиент на ту же маску и масштабирует\n",
        "\n",
        "        В режиме тестирования:\n",
        "        - Возвращает градиент без изменений\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        gradOutput: [batch_size, n_feats]\n",
        "        gradInput: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            # Используем ту же маску и масштабирование\n",
        "            self.gradInput = gradOutput * self.mask / (1 - self.p)\n",
        "        else:\n",
        "            # В режиме тестирования просто копируем градиент\n",
        "            self.gradInput = gradOutput\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Dropout(p={self.p})\""
      ],
      "metadata": {
        "id": "4zH6jSh2tRmW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Особенности реализации:\n",
        "1. Маска генерируется заново для каждого батча во время обучения\n",
        "2. Масштабирование на 1/(1-p) обеспечивает одинаковое математическое ожидание на обучении и тестировании\n",
        "3. В режиме тестирования слой не производит никаких изменений входа\n",
        "4. Та же маска используется как для прямого, так и для обратного прохода\n",
        "\n",
        "Dropout - это мощный метод регуляризации, который:\n",
        "- Предотвращает переобучение\n",
        "- Уменьшает зависимость между нейронами\n",
        "- Приводит к более надежным признакам\n",
        "- Может рассматриваться как неявный ансамбль моделей"
      ],
      "metadata": {
        "id": "KK5GSGAY4fpN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOfNSYFwkPMy"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyOwEg90kPMy"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции активации\n",
        "\n",
        "Вот полный пример нелинейности исправленного линейного блока (он же ReLU).:"
      ],
      "metadata": {
        "id": "WWl6l2py4ryk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "eKuxTP6VkPMy"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WEnRGqHkPMy"
      },
      "source": [
        "## 6. Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky ReLU - это модификация обычного ReLU, которая решает проблему \"умирающих нейронов\", добавляя небольшой наклон для отрицательных значений.\n",
        "\n",
        "Функция определяется как:\n",
        "\n",
        "$ f(x) = x $, если $x > 0$\n",
        "\n",
        "$f(x) = slope * x$, если $x ≤ 0$"
      ],
      "metadata": {
        "id": "GZgolucu46zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        \"\"\"\n",
        "        slope: наклон для отрицательных значений\n",
        "        \"\"\"\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход Leaky ReLU.\n",
        "\n",
        "        f(x) = x если x > 0\n",
        "        f(x) = slope * x если x ≤ 0\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        output: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Сохраняем маску для обратного прохода\n",
        "        self.mask = (input > 0)\n",
        "\n",
        "        # Применяем Leaky ReLU\n",
        "        self.output = np.where(self.mask,\n",
        "                             input,  # если x > 0\n",
        "                             input * self.slope)  # если x ≤ 0\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход Leaky ReLU.\n",
        "\n",
        "        f'(x) = 1 если x > 0\n",
        "        f'(x) = slope если x ≤ 0\n",
        "\n",
        "        input: [batch_size, n_feats]\n",
        "        gradOutput: [batch_size, n_feats]\n",
        "        gradInput: [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Используем сохраненную маску для определения градиента\n",
        "        self.gradInput = np.where(self.mask,\n",
        "                                gradOutput,  # если x > 0\n",
        "                                gradOutput * self.slope)  # если x ≤ 0\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"LeakyReLU(slope={self.slope})\""
      ],
      "metadata": {
        "id": "aMxvMFFh5JpX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные компоненты реализации:\n",
        "\n",
        "1. `updateOutput(self, input)`:\n",
        "   - Создаем маску для положительных значений: `input > 0`\n",
        "   - Используем np.where для применения разных преобразований:\n",
        "     - Для x > 0: оставляем как есть\n",
        "     - Для x ≤ 0: умножаем на slope\n",
        "\n",
        "2. `updateGradInput(self, input, gradOutput)`:\n",
        "   - Используем ту же маску для определения градиента:\n",
        "     - Для x > 0: градиент = 1\n",
        "     - Для x ≤ 0: градиент = slope\n",
        "\n",
        "Преимущества Leaky ReLU:\n",
        "1. Решает проблему \"умирающих нейронов\" обычного ReLU\n",
        "2. Позволяет градиенту течь через отрицательную область\n",
        "3. Сохраняет простоту вычислений\n",
        "4. Помогает в случаях, когда ReLU систематически деактивирует нейроны\n",
        "\n",
        "Рекомендации по выбору slope:\n",
        "- Типичные значения: 0.01 - 0.1\n",
        "- Меньшие значения (≈0.01) ближе к обычному ReLU\n",
        "- Большие значения (≈0.1) дают более сильный градиент для отрицательных значений\n",
        "- Можно экспериментировать со значением slope в зависимости от задачи"
      ],
      "metadata": {
        "id": "gh-oK8CQ5aSp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxeDODiYkPMy"
      },
      "source": [
        "## 7. ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELU - это функция активации, которая похожа на ReLU, но имеет экспоненциальную составляющую для отрицательных значений:\n",
        "\n",
        "f(x) = x, если x > 0\n",
        "\n",
        "f(x) = α * (exp(x) - 1), если x ≤ 0"
      ],
      "metadata": {
        "id": "iaFhak4R6Tr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        \"\"\"\n",
        "        Инициализация ELU\n",
        "\n",
        "        Параметры:\n",
        "        alpha: коэффициент масштабирования для отрицательных значений\n",
        "        \"\"\"\n",
        "        super(ELU, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход ELU.\n",
        "\n",
        "        f(x) = x если x > 0\n",
        "        f(x) = alpha * (exp(x) - 1) если x ≤ 0\n",
        "\n",
        "        Параметры:\n",
        "        input: входной тензор [batch_size, n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        output: выходной тензор [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Сохраняем маску положительных значений для обратного прохода\n",
        "        self.mask = (input > 0)\n",
        "\n",
        "        # Копируем вход для модификации\n",
        "        self.output = input.copy()\n",
        "\n",
        "        # Применяем экспоненциальную функцию только к отрицательным значениям\n",
        "        neg_mask = ~self.mask\n",
        "        self.output[neg_mask] = self.alpha * (np.exp(input[neg_mask]) - 1)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход ELU.\n",
        "\n",
        "        f'(x) = 1 если x > 0\n",
        "        f'(x) = alpha * exp(x) если x ≤ 0\n",
        "\n",
        "        Параметры:\n",
        "        input: входной тензор [batch_size, n_feats]\n",
        "        gradOutput: градиент от следующего слоя [batch_size, n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент по входу [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Инициализируем градиент\n",
        "        self.gradInput = gradOutput.copy()\n",
        "\n",
        "        # Для отрицательных значений умножаем градиент на производную ELU\n",
        "        neg_mask = ~self.mask\n",
        "        self.gradInput[neg_mask] *= self.alpha * np.exp(input[neg_mask])\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ELU(alpha={self.alpha})\""
      ],
      "metadata": {
        "id": "BWru7BAn6kRJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Подробный разбор реализации:\n",
        "\n",
        "1. Конструктор `__init__`:\n",
        "   ```python\n",
        "   def __init__(self, alpha = 1.0):\n",
        "   ```\n",
        "   - Принимает параметр `alpha`, который определяет масштаб отрицательной части\n",
        "   - По умолчанию alpha = 1.0, что является стандартным выбором\n",
        "\n",
        "2. Прямой проход `updateOutput`:\n",
        "   ```python\n",
        "   self.mask = (input > 0)  # Маска для положительных значений\n",
        "   self.output = input.copy()  # Копируем вход\n",
        "   neg_mask = ~self.mask  # Маска для отрицательных значений\n",
        "   self.output[neg_mask] = self.alpha * (np.exp(input[neg_mask]) - 1)\n",
        "   ```\n",
        "   - Создаем маску для разделения положительных и отрицательных значений\n",
        "   - Для x > 0: оставляем значения без изменений\n",
        "   - Для x ≤ 0: применяем формулу α(exp(x) - 1)\n",
        "\n",
        "3. Обратный проход `updateGradInput`:\n",
        "   ```python\n",
        "   self.gradInput = gradOutput.copy()\n",
        "   neg_mask = ~self.mask\n",
        "   self.gradInput[neg_mask] *= self.alpha * np.exp(input[neg_mask])\n",
        "   ```\n",
        "   - Для x > 0: градиент равен входному градиенту (производная = 1)\n",
        "   - Для x ≤ 0: градиент умножается на α*exp(x) (производная ELU)\n",
        "\n",
        "Преимущества ELU:\n",
        "1. Как и LeakyReLU, решает проблему \"умирающих нейронов\"\n",
        "2. Имеет гладкую функцию (непрерывная первая производная)\n",
        "3. Может давать более быструю сходимость\n",
        "4. Естественным образом приводит к нулевому среднему активаций\n",
        "\n",
        "Рекомендации по использованию:\n",
        "- Стандартное значение α = 1.0 работает хорошо в большинстве случаев\n",
        "- ELU особенно эффективен в глубоких сетях\n",
        "- Может потребовать больше вычислений из-за экспоненты\n",
        "- Хорошо работает с batch normalization"
      ],
      "metadata": {
        "id": "uRHjLL3P7TWn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbHrvkYUkPMy"
      },
      "source": [
        "## 8. SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SoftPlus - это гладкая аппроксимация функции ReLU. Она определяется как:\n",
        "\n",
        "f(x) = ln(1 + exp(x))\n",
        "\n",
        "Её производная:\n",
        "\n",
        "f'(x) = 1 / (1 + exp(-x)) = sigmoid(x)"
      ],
      "metadata": {
        "id": "7FQj34nV7uhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация SoftPlus\n",
        "        Не требует параметров\n",
        "        \"\"\"\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход SoftPlus.\n",
        "\n",
        "        f(x) = ln(1 + exp(x))\n",
        "\n",
        "        Для численной стабильности используем:\n",
        "        f(x) = x + ln(1 + exp(-x)) при x > threshold\n",
        "        f(x) = ln(1 + exp(x)) при x ≤ threshold\n",
        "\n",
        "        Параметры:\n",
        "        input: входной тензор [batch_size, n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        output: выходной тензор [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Сохраняем вход для обратного прохода\n",
        "        self.input = input\n",
        "\n",
        "        # Порог для численной стабильности\n",
        "        threshold = 20\n",
        "\n",
        "        # Маска для больших значений\n",
        "        large_values = input > threshold\n",
        "\n",
        "        # Инициализируем выход\n",
        "        self.output = np.zeros_like(input)\n",
        "\n",
        "        # Для больших значений используем стабильную формулу\n",
        "        self.output[large_values] = input[large_values] + \\\n",
        "                                  np.log(1 + np.exp(-input[large_values]))\n",
        "\n",
        "        # Для остальных значений используем обычную формулу\n",
        "        small_values = ~large_values\n",
        "        self.output[small_values] = np.log(1 + np.exp(input[small_values]))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход SoftPlus.\n",
        "\n",
        "        f'(x) = 1 / (1 + exp(-x)) = sigmoid(x)\n",
        "\n",
        "        Параметры:\n",
        "        input: входной тензор [batch_size, n_feats]\n",
        "        gradOutput: градиент от следующего слоя [batch_size, n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент по входу [batch_size, n_feats]\n",
        "        \"\"\"\n",
        "        # Вычисляем сигмоиду от входа (производная SoftPlus)\n",
        "        sigmoid = 1 / (1 + np.exp(-input))\n",
        "\n",
        "        # Умножаем входной градиент на производную\n",
        "        self.gradInput = gradOutput * sigmoid\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ],
      "metadata": {
        "id": "vvT8HISh7zDV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подробный разбор :\n",
        "\n",
        "1. Конструктор `__init__`:\n",
        "   ```python\n",
        "   def __init__(self):\n",
        "   ```\n",
        "   - Не требует параметров, так как SoftPlus - это фиксированная функция\n",
        "\n",
        "2. Прямой проход `updateOutput`:\n",
        "   ```python\n",
        "   threshold = 20  # Порог для численной стабильности\n",
        "   large_values = input > threshold\n",
        "   \n",
        "   # Для больших x используем: x + ln(1 + exp(-x))\n",
        "   self.output[large_values] = input[large_values] + \\\n",
        "                              np.log(1 + np.exp(-input[large_values]))\n",
        "   \n",
        "   # Для малых x используем: ln(1 + exp(x))\n",
        "   small_values = ~large_values\n",
        "   self.output[small_values] = np.log(1 + np.exp(input[small_values]))\n",
        "   ```\n",
        "   - Используем разные формулы для больших и малых значений для численной стабильности\n",
        "   - Для больших x используем преобразованную формулу, чтобы избежать переполнения\n",
        "\n",
        "3. Обратный проход `updateGradInput`:\n",
        "   ```python\n",
        "   # Вычисляем сигмоиду (производная SoftPlus)\n",
        "   sigmoid = 1 / (1 + np.exp(-input))\n",
        "   \n",
        "   # Умножаем градиент на производную\n",
        "   self.gradInput = gradOutput * sigmoid\n",
        "   ```\n",
        "   - Производная SoftPlus - это сигмоида\n",
        "   - Умножаем входной градиент на производную согласно правилу цепи\n",
        "\n",
        "Особенности SoftPlus:\n",
        "1. Является гладкой версией ReLU\n",
        "2. Всегда имеет положительную производную\n",
        "3. Выход всегда положителен (как у ReLU)\n",
        "4. При больших положительных x близка к x (как ReLU)\n",
        "5. При больших отрицательных x близка к 0 (как ReLU)\n",
        "\n",
        "Сравнение с ReLU:\n",
        "- Преимущества:\n",
        "  - Гладкая функция (все производные существуют)\n",
        "  - Нет резкого перехода в нуле\n",
        "  - Может быть полезна, когда нужна гладкость\n",
        "- Недостатки:\n",
        "  - Требует больше вычислений\n",
        "  - Градиент всегда меньше 1\n",
        "  - Отсутствие разреженности активаций (или отсутчвие того, что значительная часть выходов нейронов равна точно нулю)\n",
        "\n",
        "SoftPlus часто используется:\n",
        "- В вероятностных моделях\n",
        "- Когда требуется гладкость\n",
        "- В задачах, где важна непрерывность производных"
      ],
      "metadata": {
        "id": "4hTy0ihFYPmZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NebHty-TkPMz"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHv4O0sjkPMz"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Критерии используются для оценки ответов моделей."
      ],
      "metadata": {
        "id": "yszY-UIwbm2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p807wSRPkPMz"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дополнительные пояснения:\n",
        "1. Criterions (Критерии) используются для оценки качества ответов модели, то есть для вычисления функции потерь (loss function).\n",
        "\n",
        "2. Базовый класс Criterion определяет интерфейс для всех критериев с двумя основными операциями:\n",
        "   - forward: вычисление значения функции потерь\n",
        "   - backward: вычисление градиентов для обратного распространения\n",
        "\n",
        "3. Структура класса предполагает, что реальная реализация будет происходить в методах:\n",
        "   - updateOutput: реализация прямого прохода\n",
        "   - updateGradInput: реализация обратного прохода\n",
        "\n",
        "4. Методы forward и backward являются оберткам и не должны переопределяться в дочерних классах для поддержания согласованности интерфейса."
      ],
      "metadata": {
        "id": "BXMlN2Bvb88s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ybUEZKkPMz"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE (Mean Squared Error - среднеквадратичная ошибка).\n",
        "\n",
        "MSE - это базовый критерий для задач регрессии, который вычисляет среднее квадратов разностей между предсказанными и фактическими значениями."
      ],
      "metadata": {
        "id": "52nn4HxyfD0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация критерия MSE\n",
        "        Наследует базовый класс Criterion\n",
        "        \"\"\"\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Прямой проход MSE:\n",
        "        MSE = (1/n) * Σ(y_pred - y_true)²\n",
        "\n",
        "        Параметры:\n",
        "        input (y_pred): предсказания модели [batch_size x n_feats]\n",
        "        target (y_true): целевые значения [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        output: скалярное значение ошибки (усредненное по батчу)\n",
        "        \"\"\"\n",
        "        # Вычисляем квадрат разности и суммируем\n",
        "        self.output = np.sum(np.power(input - target, 2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Обратный проход MSE.\n",
        "        Производная MSE: d(MSE)/d(input) = 2(y_pred - y_true)/n\n",
        "\n",
        "        Параметры:\n",
        "        input (y_pred): предсказания модели [batch_size x n_feats]\n",
        "        target (y_true): целевые значения [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент по входу [batch_size x n_feats]\n",
        "        \"\"\"\n",
        "        # Вычисляем градиент\n",
        "        self.gradInput = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ],
      "metadata": {
        "id": "cG9sFJ_PfJ30"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разберем детально:\n",
        "\n",
        "1. Формула MSE:\n",
        "```\n",
        "MSE = (1/n) * Σ(y_pred - y_true)²\n",
        "```\n",
        "где:\n",
        "- n - размер батча (input.shape[0])\n",
        "- y_pred - предсказания модели (input)\n",
        "- y_true - истинные значения (target)\n",
        "\n",
        "2. Прямой проход (`updateOutput`):\n",
        "```python\n",
        "self.output = np.sum(np.power(input - target, 2)) / input.shape[0]\n",
        "```\n",
        "- `input - target`: вычисляем разность между предсказаниями и целевыми значениями\n",
        "- `np.power(..., 2)`: возводим разности в квадрат\n",
        "- `np.sum(...)`: суммируем все квадраты разностей\n",
        "- `/input.shape[0]`: делим на размер батча для получения среднего\n",
        "\n",
        "3. Обратный проход (`updateGradInput`):\n",
        "```python\n",
        "self.gradInput = (input - target) * 2 / input.shape[0]\n",
        "```\n",
        "- Производная MSE получается из цепного правила:\n",
        "  - d(MSE)/d(input) = 2(y_pred - y_true)/n\n",
        "- Множитель 2 появляется из производной квадрата\n",
        "- Деление на input.shape[0] из-за усреднения\n",
        "\n",
        "Особенности MSE:\n",
        "1. Квадратичная функция потерь\n",
        "   - Сильно штрафует большие отклонения\n",
        "   - Меньше штрафует малые отклонения\n",
        "\n",
        "2. Преимущества:\n",
        "   - Простая и понятная метрика\n",
        "   - Всегда неотрицательная\n",
        "   - Дифференцируема везде\n",
        "   - Выпуклая функция\n",
        "\n",
        "3. Недостатки:\n",
        "   - Чувствительна к выбросам\n",
        "   - Может замедлять обучение при больших ошибках\n",
        "\n",
        "4. Применение:\n",
        "   - Задачи регрессии\n",
        "   - Предсказание непрерывных величин\n",
        "   - Когда важно сильно штрафовать большие отклонения"
      ],
      "metadata": {
        "id": "q2fSEKfngI-b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgOQWkcOkPMz"
      },
      "source": [
        "## 9. Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Log-Likelihood (NLL) для классификации- это функция потерь, которая измеряет производительность модели, где выход представляет собой вероятности классов. Для one-hot encoded - функция упрощается до -log(p_c), где p_c - предсказанная вероятность правильного класса."
      ],
      "metadata": {
        "id": "qmVOOS-1jUR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15  # маленькая константа для численной стабильности\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация критерия NLL\n",
        "        \"\"\"\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Прямой проход NLL:\n",
        "        L = -(1/N) * Σ(t_i * log(p_i))\n",
        "        где t_i - one-hot target, p_i - предсказанные вероятности\n",
        "\n",
        "        Параметры:\n",
        "        input: предсказанные вероятности [batch_size x n_feats]\n",
        "        target: one-hot encoded цели [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        output: скалярное значение ошибки (усредненное по батчу)\n",
        "        \"\"\"\n",
        "        # Ограничиваем вероятности для избежания log(0)\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        # Вычисляем -log(p) для всех предсказаний\n",
        "        log_probs = -np.log(input_clamp)\n",
        "\n",
        "        # Так как target - one-hot, умножение на target выберет\n",
        "        # только log_prob для правильного класса\n",
        "        self.output = np.sum(log_probs * target) / input.shape[0]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Обратный проход NLL.\n",
        "        Градиент: -(1/N) * (t_i/p_i)\n",
        "\n",
        "        Параметры:\n",
        "        input: предсказанные вероятности [batch_size x n_feats]\n",
        "        target: one-hot encoded цели [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент по входу [batch_size x n_feats]\n",
        "        \"\"\"\n",
        "        # Ограничиваем вероятности для численной стабильности\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        # Градиент NLL: -target/input для каждого элемента\n",
        "        self.gradInput = -target / input_clamp / input.shape[0]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ],
      "metadata": {
        "id": "-uc9ep0VmSJa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Прямой проход (`updateOutput`):\n",
        "```python\n",
        "# Ограничиваем вероятности\n",
        "input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        " **# Вычисляем -log(p)**\n",
        "log_probs = -np.log(input_clamp)\n",
        "\n",
        " # Умножаем на target и усредняем по батчу\n",
        "self.output = np.sum(log_probs * target) / input.shape[0]\n",
        "```\n",
        "- Используем `clip` для избежания log(0)\n",
        "- Вычисляем отрицательный логарифм вероятностей\n",
        "- Умножение на one-hot target выбирает только нужные вероятности\n",
        "- Усредняем по батчу\n",
        "\n",
        "2. Обратный проход (`updateGradInput`):\n",
        "```python\n",
        "input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "self.gradInput = -target / input_clamp / input.shape[0]\n",
        "```\n",
        "- Производная -log(x) = -1/x\n",
        "- Умножаем на -target для выбора нужных классов\n",
        "- Делим на размер батча\n",
        "\n",
        "Особенности реализации:\n",
        "1. Численная стабильность:\n",
        "   - Используется `EPS = 1e-15` для предотвращения log(0)\n",
        "   - Вероятности ограничены в диапазоне [EPS, 1-EPS]\n",
        "\n",
        "2. One-hot encoding:\n",
        "   - Target содержит 1 только для правильного класса\n",
        "   - Упрощает вычисления, так как не нужно искать правильный класс\n",
        "\n",
        "3. Усреднение по батчу:\n",
        "   - Все значения делятся на размер батча\n",
        "   - Это делает функцию потерь независимой от размера батча\n",
        "\n",
        "Недостатки этой реализации:\n",
        "1. Численная нестабильность при очень малых или больших вероятностях\n",
        "2. Может давать неточные результаты при работе с softmax\n",
        "3. Лучше использовать более стабильную версию, которая работает с логитами\n",
        "\n",
        "Типичное использование:\n",
        "1. После слоя softmax в задачах классификации\n",
        "2. Когда выход модели - вероятности классов\n",
        "3. В комбинации с регуляризацией для предотвращения переобучения"
      ],
      "metadata": {
        "id": "zIm3cEC3ta8p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OxWBhYvkPMz"
      },
      "source": [
        "## 10. Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Log-Likelihood критерий.\n",
        "\n",
        "Основное отличие от предыдущей версии в том, что вход уже представляет собой логарифмы вероятностей (после log-softmax), что делает вычисления численно более стабильными."
      ],
      "metadata": {
        "id": "VJV4BXdyvICA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация стабильного NLL критерия\n",
        "        \"\"\"\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Прямой проход стабильного NLL.\n",
        "\n",
        "        L = -(1/N) * Σ(t_i * log_p_i)\n",
        "        где:\n",
        "        - t_i - one-hot target\n",
        "        - log_p_i - логарифмы вероятностей (вход уже в логарифмической форме)\n",
        "\n",
        "        Параметры:\n",
        "        input: логарифмы вероятностей [batch_size x n_feats]\n",
        "        target: one-hot encoded цели [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        output: скалярное значение ошибки (усредненное по батчу)\n",
        "        \"\"\"\n",
        "        # Так как target - one-hot, умножение на target выберет\n",
        "        # только log_prob для правильного класса\n",
        "        # Знак минус уже учтен в входных логарифмах вероятностей\n",
        "        self.output = -np.sum(input * target) / input.shape[0]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Обратный проход стабильного NLL.\n",
        "\n",
        "        Градиент: -(1/N) * t_i\n",
        "        (так как вход уже в логарифмической форме)\n",
        "\n",
        "        Параметры:\n",
        "        input: логарифмы вероятностей [batch_size x n_feats]\n",
        "        target: one-hot encoded цели [batch_size x n_feats]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент по входу [batch_size x n_feats]\n",
        "        \"\"\"\n",
        "        # Градиент - это просто масштабированный target с обратным знаком\n",
        "        self.gradInput = -target / input.shape[0]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ],
      "metadata": {
        "id": "YzhKx-eMvQJv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подробно:\n",
        "\n",
        "1. Прямой проход (`updateOutput`):\n",
        "```python\n",
        "self.output = -np.sum(input * target) / input.shape[0]\n",
        "```\n",
        "- Вход (`input`) уже содержит логарифмы вероятностей\n",
        "- `target` в one-hot кодировке выбирает нужные логарифмы\n",
        "- Делим на размер батча для усреднения\n",
        "- Знак минус нужен для получения loss (минимизируем отрицательный log-likelihood)\n",
        "\n",
        "2. Обратный проход (`updateGradInput`):\n",
        "```python\n",
        "self.gradInput = -target / input.shape[0]\n",
        "```\n",
        "- Градиент простой, так как вход уже в логарифмической форме\n",
        "- Просто масштабированный target с обратным знаком\n",
        "- Делим на размер батча для согласованности с прямым проходом\n",
        "\n",
        "Преимущества этой реализации над нестабильной версией:\n",
        "\n",
        "1. Численная стабильность:\n",
        "   - Нет вычисления логарифмов (они уже в входных данных)\n",
        "   - Нет деления на маленькие вероятности\n",
        "   - Меньше вычислительных операций\n",
        "\n",
        "2. Простота:\n",
        "   - Код короче и понятнее\n",
        "   - Меньше возможностей для ошибок\n",
        "   - Проще отлаживать\n",
        "\n",
        "3. Эффективность:\n",
        "   - Меньше операций\n",
        "   - Лучше работает с градиентным спуском\n",
        "\n",
        "Типичное использование:\n",
        "\n",
        "1. После слоя LogSoftmax:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(LogSoftmax())\n",
        "criterion = ClassNLLCriterion()\n",
        "```\n",
        "\n",
        "2. В задачах классификации:\n",
        "```python\n",
        "# Предположим, у нас есть логиты от модели\n",
        "logits = model(x)\n",
        "# Применяем LogSoftmax\n",
        "log_probs = log_softmax(logits)\n",
        "# Вычисляем loss\n",
        "loss = criterion(log_probs, targets)\n",
        "```\n",
        "\n",
        "3. С градиентным спуском:\n",
        "```python\n",
        "# Прямой проход\n",
        "loss = criterion(log_probs, targets)\n",
        "# Обратный проход\n",
        "grad = criterion.backward(log_probs, targets)\n",
        "# Обновление весов\n",
        "optimizer.step(grad)\n",
        "```\n",
        "\n",
        "Важно помнить:\n",
        "1. Вход должен быть именно логарифмами вероятностей\n",
        "2. Обычно используется вместе с LogSoftmax слоем\n",
        "3. Target должен быть в one-hot кодировке\n",
        "4. Этот критерий численно стабилен и предпочтителен над нестабильной версией"
      ],
      "metadata": {
        "id": "vEjV_F2EvcjJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T4h1nAckPMz"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSsZ194lkPM3"
      },
      "source": [
        "### SGD optimizer with momentum\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n",
        "- `state` - dict with optimizator state (used to save accumulated gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8nmNjeuskPM3"
      },
      "outputs": [],
      "source": [
        "def sgd_momentum(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "\n",
        "    var_index = 0\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "\n",
        "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
        "\n",
        "            current_var -= old_grad\n",
        "            var_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeY94GKDkPM3"
      },
      "source": [
        "## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
        "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
        "\n",
        "Formulas for optimizer:\n",
        "\n",
        "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
        "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$\n",
        "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
        "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_optimizer(variables, gradients, config, state):\n",
        "    state.setdefault('m', {})  # first moment vars\n",
        "    state.setdefault('v', {})  # second moment vars\n",
        "    state.setdefault('t', 0)   # timestamp\n",
        "    state['t'] += 1\n",
        "\n",
        "    # Проверяем наличие всех необходимых параметров\n",
        "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
        "        assert k in config, config.keys()\n",
        "\n",
        "    var_index = 0\n",
        "    # Вычисляем корректированный learning rate\n",
        "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / \\\n",
        "           (1 - config['beta1']**state['t'])\n",
        "\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            # Получаем моменты или инициализируем нулями\n",
        "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            # Обновляем первый момент (m)\n",
        "            # m = beta1 * m + (1 - beta1) * g\n",
        "            np.add(config['beta1'] * var_first_moment,\n",
        "                  (1 - config['beta1']) * current_grad,\n",
        "                  out=var_first_moment)\n",
        "\n",
        "            # Обновляем второй момент (v)\n",
        "            # v = beta2 * v + (1 - beta2) * g * g\n",
        "            np.add(config['beta2'] * var_second_moment,\n",
        "                  (1 - config['beta2']) * current_grad * current_grad,\n",
        "                  out=var_second_moment)\n",
        "\n",
        "            # Обновляем переменную\n",
        "            # var -= lr_t * m / (sqrt(v) + epsilon)\n",
        "            current_var -= lr_t * var_first_moment / \\\n",
        "                          (np.sqrt(var_second_moment) + config['epsilon'])\n",
        "\n",
        "            assert var_first_moment is state['m'].get(var_index)\n",
        "            assert var_second_moment is state['v'].get(var_index)\n",
        "            var_index += 1"
      ],
      "metadata": {
        "id": "pg_9x1Kc1OM8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравнение оптимизаторов:\n",
        "\n",
        "1. SGD с моментом:\n",
        "- Плюсы:\n",
        "  - Простая реализация\n",
        "  - Меньше гиперпараметров\n",
        "  - Хорошо работает для многих задач\n",
        "- Минусы:\n",
        "  - Единый learning rate для всех параметров\n",
        "  - Может застревать в седловых точках\n",
        "  - Требует ручной настройки learning rate\n",
        "\n",
        "2. Adam:\n",
        "- Плюсы:\n",
        "  - Адаптивный learning rate для каждого параметра\n",
        "  - Хорошо работает с разреженными градиентами\n",
        "  - Меньше проблем с выбором learning rate\n",
        "  - Эффективная коррекция смещения\n",
        "- Минусы:\n",
        "  - Более сложная реализация\n",
        "  - Больше гиперпараметров\n",
        "  - Может иметь проблемы с обобщением\n",
        "\n",
        "Ключевые различия в реализации:\n",
        "\n",
        "1. Накопление информации:\n",
        "   - SGD с моментом: один накопленный градиент\n",
        "   - Adam: два момента (среднее и дисперсия градиентов)\n",
        "\n",
        "2. Обновление параметров:\n",
        "   - SGD с моментом:\n",
        "     ```python\n",
        "     v = momentum * v + learning_rate * gradient\n",
        "     var -= v\n",
        "     ```\n",
        "   - Adam:\n",
        "     ```python\n",
        "     m = beta1 * m + (1 - beta1) * gradient\n",
        "     v = beta2 * v + (1 - beta2) * gradient^2\n",
        "     var -= lr_t * m / (sqrt(v) + epsilon)\n",
        "     ```\n",
        "\n",
        "3. Learning rate:\n",
        "   - SGD с моментом: фиксированный\n",
        "   - Adam: адаптивный для каждого параметра и корректируется по времени\n",
        "\n",
        "Типичные значения параметров:\n",
        "\n",
        "1. SGD с моментом:\n",
        "   ```python\n",
        "   config = {\n",
        "       'learning_rate': 0.01,\n",
        "       'momentum': 0.9\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. Adam:\n",
        "   ```python\n",
        "   config = {\n",
        "       'learning_rate': 0.001,\n",
        "       'beta1': 0.9,\n",
        "       'beta2': 0.999,\n",
        "       'epsilon': 1e-8\n",
        "   }\n",
        "   ```"
      ],
      "metadata": {
        "id": "rVnTDdVT1Z89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate\n",
        "\n",
        " В SGD с моментом используется один и тот же коэффициент learning rate для обновления всех параметров сети, независимо от их:\n",
        "- важности\n",
        "- масштаба\n",
        "- частоты изменения\n",
        "- градиентов\n",
        "\n",
        "В то время как Adam адаптивно подбирает индивидуальный темп обучения для каждого параметра на основе:\n",
        "- истории градиентов (первый момент)\n",
        "- их изменчивости (второй момент)\n",
        "\n",
        "Например, если у одного параметра градиенты стабильно большие, а у другого маленькие, Adam автоматически уменьшит шаг для первого и увеличит для второго. SGD же будет использовать одинаковый шаг для обоих."
      ],
      "metadata": {
        "id": "qZbbYLXZ4uf4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaAav_mFkPM3"
      },
      "source": [
        "# Layers for advanced track homework\n",
        "You **don't need** to implement it if you are working on `homework_main-basic.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD13D2zNkPM3"
      },
      "source": [
        "## 12. Conv2d [Advanced]\n",
        "- input:   **`batch_size x in_channels x h x w`**\n",
        "- output: **`batch_size x out_channels x h x w`**\n",
        "\n",
        "You should implement something like pytorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
        "\n",
        "Practical notes:\n",
        "- While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n",
        "- It may be convenient to use `skimage.util.pad` for zero-padding.\n",
        "- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension\n",
        "- Having troubles with understanding how to implement the layer?\n",
        " - Check the last year video of lecture 3 (starting from ~1:14:20)\n",
        " - May the google be with you"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "import scipy.signal\n",
        "import skimage.util\n",
        "import numpy as np\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        \"\"\"\n",
        "        Инициализация слоя Conv2d.\n",
        "\n",
        "        Параметры:\n",
        "        in_channels: количество входных каналов\n",
        "        out_channels: количество выходных каналов\n",
        "        kernel_size: размер ядра свертки (нечетное число)\n",
        "        \"\"\"\n",
        "        super(Conv2d, self).__init__()\n",
        "        assert kernel_size % 2 == 1, kernel_size\n",
        "\n",
        "        # Инициализация весов и смещений\n",
        "        stdv = 1./np.sqrt(in_channels)\n",
        "        self.W = np.random.uniform(-stdv, stdv,\n",
        "                                 size=(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Градиенты\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход свертки.\n",
        "\n",
        "        input: [batch_size, in_channels, h, w]\n",
        "        output: [batch_size, out_channels, h, w]\n",
        "        \"\"\"\n",
        "        pad_size = self.kernel_size // 2\n",
        "        batch_size = input.shape[0]\n",
        "        h, w = input.shape[2], input.shape[3]\n",
        "\n",
        "        # Добавляем zero-padding к входу\n",
        "        padded_input = np.pad(input,\n",
        "                            ((0,0), (0,0), (pad_size,pad_size), (pad_size,pad_size)),\n",
        "                            mode='constant')\n",
        "\n",
        "        # Инициализируем выходной тензор\n",
        "        self.output = np.zeros((batch_size, self.out_channels, h, w))\n",
        "\n",
        "        # Выполняем свертку для каждого образца в батче и каждого выходного канала\n",
        "        for i in range(batch_size):\n",
        "            for j in range(self.out_channels):\n",
        "                # Суммируем результаты свертки по всем входным каналам\n",
        "                conv_sum = np.zeros((h, w))\n",
        "                for k in range(self.in_channels):\n",
        "                    conv_sum += sp.signal.correlate2d(padded_input[i,k],\n",
        "                                                    self.W[j,k],\n",
        "                                                    mode='valid')\n",
        "                self.output[i,j] = conv_sum + self.b[j]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход - вычисление градиента по входу.\n",
        "\n",
        "        input: [batch_size, in_channels, h, w]\n",
        "        gradOutput: [batch_size, out_channels, h, w]\n",
        "        gradInput: [batch_size, in_channels, h, w]\n",
        "        \"\"\"\n",
        "        pad_size = self.kernel_size // 2\n",
        "        batch_size = input.shape[0]\n",
        "\n",
        "        # Инициализируем градиент по входу\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "\n",
        "        # Добавляем padding к градиенту выхода\n",
        "        padded_gradOutput = np.pad(gradOutput,\n",
        "                                 ((0,0), (0,0), (pad_size,pad_size), (pad_size,pad_size)),\n",
        "                                 mode='constant')\n",
        "\n",
        "        # Вычисляем градиент для каждого образца и входного канала\n",
        "        for i in range(batch_size):\n",
        "            for k in range(self.in_channels):\n",
        "                grad_sum = np.zeros_like(self.gradInput[i,k])\n",
        "                for j in range(self.out_channels):\n",
        "                    # Поворачиваем ядро на 180 градусов для свертки\n",
        "                    kernel_rotated = np.rot90(self.W[j,k], 2)\n",
        "                    grad_sum += sp.signal.correlate2d(padded_gradOutput[i,j],\n",
        "                                                    kernel_rotated,\n",
        "                                                    mode='valid')\n",
        "                self.gradInput[i,k] = grad_sum\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Вычисление градиентов по параметрам (веса и смещения).\n",
        "\n",
        "        input: [batch_size, in_channels, h, w]\n",
        "        gradOutput: [batch_size, out_channels, h, w]\n",
        "        \"\"\"\n",
        "        pad_size = self.kernel_size // 2\n",
        "        batch_size = input.shape[0]\n",
        "\n",
        "        # Добавляем padding к входу\n",
        "        padded_input = np.pad(input,\n",
        "                             ((0,0), (0,0), (pad_size,pad_size), (pad_size,pad_size)),\n",
        "                             mode='constant')\n",
        "\n",
        "        # Вычисляем градиенты весов\n",
        "        for j in range(self.out_channels):\n",
        "            for k in range(self.in_channels):\n",
        "                grad_sum = np.zeros_like(self.W[j,k])\n",
        "                for i in range(batch_size):\n",
        "                    grad_sum += sp.signal.correlate2d(padded_input[i,k],\n",
        "                                                    gradOutput[i,j],\n",
        "                                                    mode='valid')\n",
        "                self.gradW[j,k] = grad_sum\n",
        "\n",
        "        # Вычисляем градиенты смещений\n",
        "        self.gradb = np.sum(gradOutput, axis=(0,2,3))\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"Обнуляем градиенты\"\"\"\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"Возвращаем параметры слоя\"\"\"\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"Возвращаем градиенты параметров\"\"\"\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        return f'Conv2d {s[1]} -> {s[0]}'"
      ],
      "metadata": {
        "id": "aTf7L9lS6YWB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ключевое:\n",
        "\n",
        "1. Прямой проход (`updateOutput`):\n",
        "```python\n",
        "  # Добавляем padding\n",
        "padded_input = np.pad(input,\n",
        "                     ((0,0), (0,0), (pad_size,pad_size), (pad_size,pad_size)),\n",
        "                     mode='constant')\n",
        "\n",
        " # Для каждого образца и выходного канала\n",
        "for i in range(batch_size):\n",
        "    for j in range(self.out_channels):\n",
        "        conv_sum = np.zeros((h, w))\n",
        "        for k in range(self.in_channels):\n",
        "            conv_sum += sp.signal.correlate2d(padded_input[i,k],\n",
        "                                            self.W[j,k],\n",
        "                                            mode='valid')\n",
        "        self.output[i,j] = conv_sum + self.b[j]\n",
        "```\n",
        "\n",
        "2. Обратный проход по входу (`updateGradInput`):\n",
        "```python\n",
        "# Для каждого образца и входного канала\n",
        "for i in range(batch_size):\n",
        "    for k in range(self.in_channels):\n",
        "        grad_sum = np.zeros_like(self.gradInput[i,k])\n",
        "        for j in range(self.out_channels):\n",
        "            kernel_rotated = np.rot90(self.W[j,k], 2)\n",
        "            grad_sum += sp.signal.correlate2d(padded_gradOutput[i,j],\n",
        "                                            kernel_rotated,\n",
        "                                            mode='valid')\n",
        "        self.gradInput[i,k] = grad_sum\n",
        "```\n",
        "\n",
        "3. Вычисление градиентов параметров (`accGradParameters`):\n",
        "```python\n",
        "# Градиенты весов\n",
        "for j in range(self.out_channels):\n",
        "    for k in range(self.in_channels):\n",
        "        grad_sum = np.zeros_like(self.W[j,k])\n",
        "        for i in range(batch_size):\n",
        "            grad_sum += sp.signal.correlate2d(padded_input[i,k],\n",
        "                                            gradOutput[i,j],\n",
        "                                            mode='valid')\n",
        "        self.gradW[j,k] = grad_sum\n",
        "\n",
        " # Градиенты смещений\n",
        "self.gradb = np.sum(gradOutput, axis=(0,2,3))\n",
        "```\n",
        "\n",
        "Важные особенности:\n",
        "1. Используется корреляция вместо свертки\n",
        "2. Добавляется zero-padding для сохранения размерности\n",
        "3. Отдельно обрабатывается каждый канал\n",
        "4. Учитываются смещения (bias) для каждого выходного канала"
      ],
      "metadata": {
        "id": "uYK0JDu_6f_U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFqxpItPkPM4"
      },
      "source": [
        "## 13. MaxPool2d [Advanced]\n",
        "- input:   **`batch_size x n_input_channels x h x w`**\n",
        "- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n",
        "\n",
        "You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n",
        "\n",
        "Practical notes:\n",
        "- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n",
        "- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n",
        "- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gIyDjkRlYcGs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(Module):\n",
        "   def __init__(self, kernel_size):\n",
        "       \"\"\"\n",
        "       Инициализация MaxPool2d\n",
        "       kernel_size: размер окна для пулинга\n",
        "       \"\"\"\n",
        "       super(MaxPool2d, self).__init__()\n",
        "       self.kernel_size = kernel_size\n",
        "       self.gradInput = None\n",
        "\n",
        "   def updateOutput(self, input):\n",
        "       \"\"\"\n",
        "       Прямой проход MaxPool2d\n",
        "\n",
        "       input: [batch_size, channels, height, width]\n",
        "       output: [batch_size, channels, height/kernel_size, width/kernel_size]\n",
        "       \"\"\"\n",
        "       # Получаем размеры входа\n",
        "       batch_size, channels, input_h, input_w = input.shape\n",
        "\n",
        "       # Проверяем, что размеры делятся на размер ядра\n",
        "       assert input_h % self.kernel_size == 0\n",
        "       assert input_w % self.kernel_size == 0\n",
        "\n",
        "       # Вычисляем выходные размеры\n",
        "       output_h = input_h // self.kernel_size\n",
        "       output_w = input_w // self.kernel_size\n",
        "\n",
        "       # Преобразуем вход для удобного пулинга\n",
        "       # [batch, channel, h, w] ->\n",
        "       # [batch, channel, h/k, k, w/k, k]\n",
        "       x_reshaped = input.reshape(batch_size, channels,\n",
        "                                output_h, self.kernel_size,\n",
        "                                output_w, self.kernel_size)\n",
        "\n",
        "       # Меняем оси для группировки пространственных измерений\n",
        "       # [batch, channel, h/k, k, w/k, k] ->\n",
        "       # [batch, channel, h/k, w/k, k, k]\n",
        "       x_reordered = x_reshaped.transpose(0, 1, 2, 4, 3, 5)\n",
        "\n",
        "       # Получаем вид для операции максимума\n",
        "       x_pooling = x_reordered.reshape(*x_reordered.shape[:-2], -1)\n",
        "\n",
        "       # Находим индексы максимумов\n",
        "       self.max_indices = x_pooling.argmax(axis=-1)\n",
        "\n",
        "       # Находим максимальные значения\n",
        "       self.output = x_pooling.max(axis=-1)\n",
        "\n",
        "       return self.output\n",
        "\n",
        "   def updateGradInput(self, input, gradOutput):\n",
        "       \"\"\"\n",
        "       Обратный проход MaxPool2d\n",
        "\n",
        "       input: [batch_size, channels, height, width]\n",
        "       gradOutput: [batch_size, channels, height/kernel_size, width/kernel_size]\n",
        "       gradInput: [batch_size, channels, height, width]\n",
        "       \"\"\"\n",
        "       batch_size, channels, input_h, input_w = input.shape\n",
        "       output_h = input_h // self.kernel_size\n",
        "       output_w = input_w // self.kernel_size\n",
        "       k = self.kernel_size\n",
        "\n",
        "       # Создаем градиент входа нужной формы\n",
        "       self.gradInput = np.zeros_like(input)\n",
        "       grad_reshaped = self.gradInput.reshape(batch_size, channels,\n",
        "                                            output_h, k, output_w, k)\n",
        "\n",
        "       # Создаем индексы для распределения градиентов\n",
        "       b_idx = np.repeat(np.arange(batch_size), channels * output_h * output_w)\n",
        "       c_idx = np.tile(np.repeat(np.arange(channels), output_h * output_w), batch_size)\n",
        "       h_idx = np.tile(np.repeat(np.arange(output_h), output_w), batch_size * channels)\n",
        "       w_idx = np.tile(np.arange(output_w), batch_size * channels * output_h)\n",
        "\n",
        "       # Преобразуем линейные индексы в координаты в окне пулинга\n",
        "       pool_h = self.max_indices // k\n",
        "       pool_w = self.max_indices % k\n",
        "\n",
        "       # Распределяем градиенты в позиции максимумов\n",
        "       flat_grad = gradOutput.ravel()\n",
        "       grad_reshaped[b_idx, c_idx, h_idx, pool_h.ravel(),\n",
        "                    w_idx, pool_w.ravel()] = flat_grad\n",
        "\n",
        "       return self.gradInput\n",
        "\n",
        "   def __repr__(self):\n",
        "       return f'MaxPool2d, kern {self.kernel_size}, stride {self.kernel_size}'"
      ],
      "metadata": {
        "id": "DvdZYPhOV-HR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Разберем ключевые моменты реализации:\n",
        "\n",
        "1. Прямой проход (`updateOutput`):\n",
        "```python\n",
        "# Преобразование входа\n",
        "x_reshaped = input.reshape(batch_size, channels,\n",
        "                          output_h, self.kernel_size,\n",
        "                          output_w, self.kernel_size)\n",
        "x_reordered = x_reshaped.transpose(0, 1, 2, 4, 3, 5)\n",
        "\n",
        " # Максимумы и их индексы\n",
        "self.output = x_reordered.max(axis=(4, 5))\n",
        "self.max_indices = x_reordered.reshape(batch_size, channels,\n",
        "                                     output_h, output_w, -1).argmax(axis=-1)\n",
        "```\n",
        "\n",
        "2. Обратный проход (`updateGradInput`):\n",
        "```python\n",
        "  # Инициализация градиента\n",
        "self.gradInput = np.zeros_like(input)\n",
        "grad_input_reshaped = self.gradInput.reshape(batch_size, channels,\n",
        "                                           output_h, self.kernel_size,\n",
        "                                           output_w, self.kernel_size)\n",
        "\n",
        " # Распределение градиентов\n",
        "kernel_h = self.max_indices // self.kernel_size\n",
        "kernel_w = self.max_indices % self.kernel_size\n",
        "grad_input_reshaped[batch_idx, channel_idx, height_idx, kernel_h,\n",
        "                   width_idx, kernel_w] = gradOutput\n",
        "```\n",
        "\n",
        "Особенности реализации:\n",
        "\n",
        "1. Space-to-batch преобразование:\n",
        "   - Переформатирует входные данные для эффективного пулинга\n",
        "   - Позволяет использовать векторизованные операции numpy\n",
        "\n",
        "2. Сохранение индексов:\n",
        "   - Запоминаем позиции максимальных значений\n",
        "   - Используем их при обратном проходе\n",
        "   - Эффективнее, чем повторный поиск максимумов\n",
        "\n",
        "3. Ограничения:\n",
        "   - Шаг равен размеру ядра\n",
        "   - Входные размеры должны делиться на размер ядра\n",
        "   - Нет поддержки padding\n",
        "\n",
        "4. Векторизация:\n",
        "   - Использование numpy для эффективных вычислений\n",
        "   - Минимум циклов в коде\n",
        "\n",
        "Типичное использование:\n",
        "```python\n",
        " # Создание слоя\n",
        "pool = MaxPool2d(kernel_size=2)\n",
        "\n",
        " # Прямой проход\n",
        "x = np.random.randn(32, 64, 28, 28)  # [batch, channels, height, width]\n",
        "out = pool.updateOutput(x)  # [32, 64, 14, 14]\n",
        "\n",
        " # Обратный проход\n",
        "grad_output = np.random.randn(32, 64, 14, 14)\n",
        "grad_input = pool.updateGradInput(x, grad_output)  # [32, 64, 28, 28]\n",
        "```"
      ],
      "metadata": {
        "id": "nGcvxwkC7aPG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1CVp35WkPM4"
      },
      "source": [
        "### Flatten layer\n",
        "Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сглаживающий слой\n",
        "\n",
        "Просто изменяет форму входных данных и градиентов. Обычно используется в качестве промежуточного слоя между Conv2d и Linear.\n"
      ],
      "metadata": {
        "id": "JVTJmEVX7xSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализация слоя Flatten.\n",
        "        Не требует параметров.\n",
        "        \"\"\"\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Прямой проход: преобразует вход в двумерный массив.\n",
        "\n",
        "        Например:\n",
        "        [batch_size, channels, height, width] -> [batch_size, channels*height*width]\n",
        "\n",
        "        Параметры:\n",
        "        input: многомерный массив с первым измерением batch_size\n",
        "\n",
        "        Возвращает:\n",
        "        output: двумерный массив [batch_size, features]\n",
        "        \"\"\"\n",
        "        # Сохраняем размер батча (первое измерение)\n",
        "        # -1 автоматически вычисляет второе измерение\n",
        "        self.output = input.reshape(len(input), -1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Обратный проход: восстанавливает исходную форму градиента.\n",
        "\n",
        "        Параметры:\n",
        "        input: исходный вход (нужен только для получения формы)\n",
        "        gradOutput: градиент в плоской форме [batch_size, features]\n",
        "\n",
        "        Возвращает:\n",
        "        gradInput: градиент в исходной форме input.shape\n",
        "        \"\"\"\n",
        "        # Восстанавливаем исходную форму для градиента\n",
        "        self.gradInput = gradOutput.reshape(input.shape)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ],
      "metadata": {
        "id": "_wgCgu367_sb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда использовать:\n",
        "\n",
        "\n",
        "*   После сверточных слоев перед полносвязными\n",
        "*  Когда нужно преобразовать многомерные данные в вектор\n",
        "*  В архитектурах типа CNN + MLP\n",
        "\n",
        "\n",
        "Важные замечания:\n",
        "\n",
        "* Сохраняет размер батча (первое измерение)\n",
        "* Объединяет все остальные измерения в одно\n",
        "* Не влияет на значения, только на форму данных\n",
        "* Является полностью обратимым преобразованием"
      ],
      "metadata": {
        "id": "_oEgxYrT8R5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "id": "bfh5zbgFkPM5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}